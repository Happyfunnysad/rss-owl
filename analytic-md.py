import sqlite3
from datetime import datetime, timedelta
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
import re
import seaborn as sns
from wordcloud import WordCloud
import numpy as np
import os

class TelegramAnalyzer:
    def __init__(self, db_path='tg-posts.db'):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        
        # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã
        self.df = pd.read_sql_query("""
            SELECT 
                id,
                post_id,
                content,
                strftime('%Y-%m-%d %H:%M:%S', published_date) as published_date,
                source_url,
                strftime('%Y-%m-%d %H:%M:%S', created_at) as created_at
            FROM posts
        """, self.conn)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç—ã
        self.df['published_date'] = pd.to_datetime(self.df['published_date'])
        self.df['created_at'] = pd.to_datetime(self.df['created_at'])

        # –ü–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –¥–æ–±–∞–≤–ª—è–µ–º UTC –∑–æ–Ω—É
        self.df['published_date'] = self.df['published_date'].dt.tz_localize('UTC')
        self.df['created_at'] = self.df['created_at'].dt.tz_localize('UTC')

    def basic_stats(self):
        """–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ—Å—Ç–∞–º"""
        stats = {
            '–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤': len(self.df),
            '–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤': len(self.df['source_url'].unique()),
            '–ü–µ—Ä–≤—ã–π –ø–æ—Å—Ç': self.df['published_date'].min(),
            '–ü–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ—Å—Ç': self.df['published_date'].max(),
            '–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–æ—Å—Ç–∞ (—Å–∏–º–≤–æ–ª–æ–≤)': self.df['content'].str.len().mean(),
            '–ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å—Ç–∞': self.df['content'].str.len().median(),
        }
        return stats

    def posts_by_channel(self):
        """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ –ø–æ –∫–∞–Ω–∞–ª–∞–º"""
        channels = self.df['source_url'].apply(lambda x: x.split('/')[-1])
        return channels.value_counts()

    def posts_by_hour(self):
        """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ –ø–æ —á–∞—Å–∞–º"""
        return self.df['published_date'].dt.hour.value_counts().sort_index()

    def posts_by_date(self):
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –ø–æ –¥–∞—Ç–∞–º"""
        return self.df.groupby(self.df['published_date'].dt.date).size()

    def word_frequency(self, min_length=4):
        """–ß–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –ø–æ—Å—Ç–∞—Ö"""
        words = ' '.join(self.df['content']).lower()
        words = re.findall(r'\b\w+\b', words)
        return Counter([w for w in words if len(w) >= min_length])

    def alert_keywords_analysis(self):
        """–ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Ç—Ä–µ–≤–æ–≥–∏"""
        keywords = ['—Ç—Ä–µ–≤–æ–≥–∞', '–≤–Ω–∏–º–∞–Ω–∏–µ', '–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '—É–≥—Ä–æ–∑–∞', '—Å—Ä–æ—á–Ω–æ' , 'fpv' , '–±–ø–ª–∞' , '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä' , '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä—Å–∫–∏–π –∫—Ä–∞–π'  , '–†–æ—Å—Ç–æ–≤' , '–†–æ—Å—Ç–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å']
        stats = {}
        for keyword in keywords:
            mask = self.df['content'].str.contains(keyword, case=False)
            stats[keyword] = {
                'total_mentions': mask.sum(),
                'percentage': (mask.sum() / len(self.df)) * 100
            }
        return stats

    def response_time_analysis(self):
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –º–µ–∂–¥—É –ø—É–±–ª–∏–∫–∞—Ü–∏–µ–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º"""
        self.df['response_time'] = (self.df['created_at'] - self.df['published_date'])
        return {
            'mean_response': self.df['response_time'].mean(),
            'median_response': self.df['response_time'].median(),
            'min_response': self.df['response_time'].min(),
            'max_response': self.df['response_time'].max()
        }

    def location_analysis(self):
        """–ê–Ω–∞–ª–∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –≥–æ—Ä–æ–¥–æ–≤ –∏ –æ–±–ª–∞—Å—Ç–µ–π"""
        locations = {
            # –û–±–ª–∞—Å—Ç–∏
            '–æ–±–ª–∞—Å—Ç–∏': {
                '–ë—Ä—è–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': ['–ë—Ä—è–Ω—Å–∫', '–°—Ç–∞—Ä–æ–¥—É–±', '–ö–ª–∏–º–æ–≤–æ', '–ù–∞–≤–ª—è'],
                '–ö—É—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': ['–ö—É—Ä—Å–∫', '–û–±–æ—è–Ω—å'],
                '–ë–µ–ª–≥–æ—Ä–æ–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': ['–ë–µ–ª–≥–æ—Ä–æ–¥', '–í–∞–ª—É–π–∫–∏', '–ë–æ—Ä–∏—Å–æ–≤–∫–∞', '–Ø—Å–Ω—ã–µ –ó–æ—Ä–∏'],
                '–†–æ—Å—Ç–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': ['–†–æ—Å—Ç–æ–≤', '–¢–∞–≥–∞–Ω—Ä–æ–≥'],
                '–û—Ä–ª–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': ['–û—Ä—ë–ª'],
                '–ö–∞–ª—É–∂—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': ['–ö–∞–ª—É–≥–∞'],
                '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä—Å–∫–∏–π –∫—Ä–∞–π': ['–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä', '–ï–π—Å–∫', '–°–ª–∞–≤—è–Ω—Å–∫-–Ω–∞-–ö—É–±–∞–Ω–∏', '–ö—Ä—ã–º—Å–∫', '–¢–µ–º—Ä—é–∫'],
            },
            # –û—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏
            '—Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏': {
                '–õ–î–ù–†': ['–î–ù–†', '–õ–ù–†', '–î–æ–Ω–µ—Ü–∫', '–ì–æ—Ä–ª–æ–≤–∫–∞', '–ï–Ω–∞–∫–∏–µ–≤–æ', '–í–æ–ª–Ω–æ–≤–∞—Ö–∞', 
                        '–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–æ–≤–∫–∞', '–ü–æ–∫—Ä–æ–≤—Å–∫', '–õ—é–±–∏–º–æ–≤–∫–∞'],
                '–ü—Ä–∏–∞–∑–æ–≤—å–µ': ['–ë–µ—Ä–¥—è–Ω—Å–∫', '–ú–∞—Ä–∏—É–ø–æ–ª—å'],
            }
        }
        
        stats = {
            '–æ–±–ª–∞—Å—Ç–∏': {},
            '—Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏': {},
            '–≥–æ—Ä–æ–¥–∞': {}
        }
        
        # –ü–æ–¥—Å—á–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –æ–±–ª–∞—Å—Ç–µ–π –∏ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π
        for category in ['–æ–±–ª–∞—Å—Ç–∏', '—Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏']:
            for region, cities in locations[category].items():
                # –°—á–∏—Ç–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ä–µ–≥–∏–æ–Ω–∞
                region_mask = self.df['content'].str.contains(region, case=False)
                region_mentions = region_mask.sum()
                
                # –°—á–∏—Ç–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤ —Ä–µ–≥–∏–æ–Ω–∞
                cities_mentions = {}
                for city in cities:
                    city_mask = self.df['content'].str.contains(city, case=False)
                    city_mentions = city_mask.sum()
                    if city_mentions > 0:
                        cities_mentions[city] = {
                            'total_mentions': city_mentions,
                            'percentage': (city_mentions / len(self.df)) * 100
                        }
                
                if region_mentions > 0 or cities_mentions:
                    stats[category][region] = {
                        'total_mentions': region_mentions,
                        'percentage': (region_mentions / len(self.df)) * 100,
                        'cities': cities_mentions
                    }
        
        return stats

    def generate_plots(self, output_dir='analytics'):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Å—Ç–æ–≤ –ø–æ –¥–∞—Ç–∞–º
        plt.figure(figsize=(15, 5))
        self.posts_by_date().plot(kind='bar')
        plt.title('–ü–æ—Å—Ç—ã –ø–æ –¥–∞—Ç–∞–º')
        plt.tight_layout()
        plt.savefig(f'{output_dir}/posts_by_date.png')
        plt.close()

        # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Å—Ç–æ–≤ –ø–æ —á–∞—Å–∞–º
        plt.figure(figsize=(10, 5))
        self.posts_by_hour().plot(kind='bar')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ –ø–æ —á–∞—Å–∞–º')
        plt.tight_layout()
        plt.savefig(f'{output_dir}/posts_by_hour.png')
        plt.close()

        # –û–±–ª–∞–∫–æ —Å–ª–æ–≤
        wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(
            ' '.join(self.df['content'])
        )
        plt.figure(figsize=(20,10))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('on')
        plt.tight_layout(pad=0)
        plt.savefig(f'{output_dir}/wordcloud.png')
        plt.close()

    def export_report(self, output_dir='analytics'):
        """–≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞ –≤ markdown –≤ –ø–∞–ø–∫—É analytics"""
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, 'analytics_report.md')
        
        basic = self.basic_stats()
        alerts = self.alert_keywords_analysis()
        response = self.response_time_analysis()
        locations = self.location_analysis()
        
        report = f"""# –ê–Ω–∞–ª–∏–∑ Telegram –ø–æ—Å—Ç–æ–≤
        
## üìä –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- üìù –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {basic['–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤']}
- üì¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤: {basic['–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤']}
- üìÖ –ü–µ—Ä–∏–æ–¥: —Å {basic['–ü–µ—Ä–≤—ã–π –ø–æ—Å—Ç']} –ø–æ {basic['–ü–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ—Å—Ç']}
- üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–æ—Å—Ç–∞: {basic['–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–æ—Å—Ç–∞ (—Å–∏–º–≤–æ–ª–æ–≤)']:.1f} —Å–∏–º–≤–æ–ª–æ–≤

## üìà –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Ç—Ä–µ–≤–æ–≥–∏
"""
        for word, stats in alerts.items():
            report += f"- {word}: {stats['total_mentions']} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π ({stats['percentage']:.1f}%)\n"

        report += "\n## üìç –ê–Ω–∞–ª–∏–∑ –ª–æ–∫–∞—Ü–∏–π\n"
        for category, regions in locations.items():
            report += f"\n### {category.title()}\n"
            for region, data in regions.items():
                report += f"\n#### {region}\n"
                report += f"- –í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: {data['total_mentions']} ({data['percentage']:.1f}%)\n"
                
                if data['cities']:
                    report += "- –ì–æ—Ä–æ–¥–∞:\n"
                    for city, city_data in data['cities'].items():
                        report += f"  - {city}: {city_data['total_mentions']} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π "
                        report += f"({city_data['percentage']:.1f}%)\n"

        report += f"""
## ‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
- ‚åõ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {response['mean_response']}
- üìä –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {response['median_response']}
- ‚ö° –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {response['min_response']}
- üïí –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {response['max_response']}

## üìä –ì—Ä–∞—Ñ–∏–∫–∏
- [–ü–æ—Å—Ç—ã –ø–æ –¥–∞—Ç–∞–º](posts_by_date.png)
- [–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —á–∞—Å–∞–º](posts_by_hour.png)
- [–û–±–ª–∞–∫–æ —Å–ª–æ–≤](wordcloud.png)

---
*–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"üìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_file}")

if __name__ == "__main__":
    analyzer = TelegramAnalyzer()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –≥—Ä–∞—Ñ–∏–∫–∏
    analyzer.generate_plots()
    analyzer.export_report()
    
    print("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞–ø–∫—É 'analytics' –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ —Ñ–∞–π–ª 'analytics_report.md' –¥–ª—è –æ—Ç—á–µ—Ç–∞.")
